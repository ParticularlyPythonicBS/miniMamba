# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_model.ipynb.

# %% auto 0
__all__ = ['random_SSM', 'discretize', 'scan_SSM', 'run_SSM', 'MambaBlock', 'RMSnorm', 'ResidualBlock', 'Mamba']

# %% ../nbs/01_model.ipynb 4
import numpy as np
import jax
import jax.numpy as jnp
import jax.numpy.linalg as jla
import equinox as eqx

from .modelArgs import ModelArgs

# %% ../nbs/01_model.ipynb 7
def random_SSM(rng, N):
    a_r, b_r, c_r = jax.random.split(rng, 3)
    A = jax.random.uniform(a_r, (N, N))
    B = jax.random.uniform(b_r, (N, 1))
    C = jax.random.uniform(c_r, (1, N))
    return A, B, C

# %% ../nbs/01_model.ipynb 9
def discretize(A, B, C, step):
    I = jnp.eye(A.shape[0])
    BL = jla.inv(I - (step / 2.0) * A)
    Ab = BL @ (I + (step / 2.0) * A)
    Bb = (BL * step) @ B
    return Ab, Bb, C

# %% ../nbs/01_model.ipynb 11
def scan_SSM(Ab, Bb, Cb, u, x0):
    def step(x_k_1, u_k):
        x_k = Ab @ x_k_1 + Bb @ u_k
        y_k = Cb @ x_k
        return x_k, y_k

    return jax.lax.scan(step, x0, u)

# %% ../nbs/01_model.ipynb 13
def run_SSM(A, B, C, u):
    L = u.shape[0]
    N = A.shape[0]
    Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L)

    # Run recurrence
    return scan_SSM(Ab, Bb, Cb, u[:, np.newaxis], np.zeros((N,)))[1]

# %% ../nbs/01_model.ipynb 15
class MambaBlock(eqx.Module):
    args: ModelArgs
    in_proj: eqx.nn.Linear
    conv1d: eqx.nn.Conv
    x_proj: eqx.nn.Linear
    dt_proj: eqx.nn.Linear
    out_proj: eqx.nn.Linear
    log_A: jnp.ndarray
    D: jnp.ndarray
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.in_proj = eqx.nn.Linear(args.d_model, args.d_inner*2, use_bias=args.bias, key = rng)

        self.conv1d = eqx.nn.Conv1D(in_channels=args.d_inner, 
                                 out_channels=args.d_inner, 
                                 kernel_size=args.d_conv,
                                 groups=args.d_inner,
                                 use_bias= args.conv_bias,
                                 key = rng)
        # maps x to (Δ, B, C)
        self.x_proj = eqx.nn.Linear(args.d_inner, args.dt_rank+args.d_state*2, use_bias=False, key = rng)

        # projects Δ from dt_rank to d_in
        self.dt_proj = eqx.nn.Linear(args.dt_rank, args.dt_inner, use_bias=True, key = rng)
        
        self.out_proj = eqx.nn.Linear(args.d_inner, args.d_model, use_bias=args.bias, key = rng)

        A = jnp.tile(jnp.arange(1, args.d_state + 1), (args.inner, 1))
        self.log_A = jnp.log(A)
        self.D = jnp.ones((args.d_inner, args.d_inner))

    def forward(self, x):
        """
        Mamba forward pass, looks like figure 3
        (seq_len, d_model) -> (seq_len, d_model)
        """
        (L,d) = x.shape

        x_and_res = self.in_proj(x) # (L, d_inner*2)
        (x, res) = jnp.split(x_and_res, 2, axis=-1) # (L, d_inner)
        x = self.conv1d(x.T)[:,:,:, :L].T # (L, d_inner)
        x = jax.nn.silu(x)

        y = self.ssm(x)
        y *= jax.nn.silu(res)
        
        output = self.out_proj(y)

        return output
    
    def ssm(self, x):
        """
        Run SSM
        """
        (d_in, n) = self.A_log.shape

        # Compute ∆ A B C D, the state space parameters.
        
        A = -jnp.exp(self.A_log) # (d_in, n)
        D = self.D

        x_dbl = self.x_proj(x) # (l, d_rank+ d_state*2)
        (delta, B, C) = jnp.split(x_dbl, [self.args.dt_rank, self.args.d_state,
                                          self.args.d_state], axis=-1) # (1, d_rank), (l, d_state), (l, d_state)
        delta = jax.nn.softplus(self.dt_proj(delta)) # (l, d_inner)

        y = self.selective_scan(x, delta, A, B, C, D)

        return y

    def selective_scan(self, u, delta, A, B, C, D):
        """ 
        Discretize and Selective scan
        """
        (L, d_in) = u.shape
        n = A.shape[1]

        # Discretizing continouis parameters acc eq 4
        deltaA = jnp.exp(jnp.einsum('ld, dn -> ldn', delta, A))
        deltaB_u = jnp.einsum('ld,ln, ld->ldn', delta, B * u)

        # perform selective scan
        y = scan_SSM(deltaA, deltaB_u, C, u, jnp.zeros((n,)))

        y += u*D
    
    def scan_SSM(Ab, Bb, Cb, u, x0):
        def step(x_k_1, u_k):
            x_k = Ab @ x_k_1 + Bb @ u_k
            y_k = Cb @ x_k
            return x_k, y_k

        return jax.lax.scan(step, x0, u)



# %% ../nbs/01_model.ipynb 16
class RMSnorm(eqx.Module):
    eps: float
    weight: jnp.ndarray
    def __init__(self, d_model: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = jnp.ones((d_model,))

    def __call__(self, x):
        output = x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps) * self.weight
        return output

# %% ../nbs/01_model.ipynb 19
class ResidualBlock(eqx.Module):
    args: ModelArgs
    mixer: MambaBlock
    norm: RMSnorm
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.mixer = MambaBlock(args)
        self.norm = RMSnorm(args.d_model)

    def forward(self, x):
        self.mixer(self.norm(x)) + x

# %% ../nbs/01_model.ipynb 20
class Mamba(eqx.Module):
    args: ModelArgs
    embedding: eqx.nn.Embedding
    layers: list
    norm_f: RMSnorm
    lm_head: eqx.nn.Linear
    def __init__(self, args: ModelArgs):
        self.args = args

        self.embedding = eqx.nn.Embedding(args.vocab_size, args.d_model, key = rng)
        self.layers = [ResidualBlock(args) for _ in range(args.n_layers)]
        self.norm = RMSnorm(args.d_model)

        self.lm_head = eqx.nn.Linear(args.d_model, args.vocab_size, use_bias=False, key = rng)

        self.lm_head.weight = self.embedding.weight # Tying weights

    def forward(self, input_ids):
        """
        l -> (l, vocab_size)
        """
        x = self.embedding(input_ids)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        x = self.lm_head(x)
        return x
