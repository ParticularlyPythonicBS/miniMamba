[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "miniMamba",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "miniMamba",
    "section": "Install",
    "text": "Install\npip install miniMamba"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "miniMamba",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "modelargs.html",
    "href": "modelargs.html",
    "title": "ModelArgs",
    "section": "",
    "text": "source\n\nModelArgs\n\n ModelArgs (d_model:int, n_layer:int, vocab_size:int, d_state:int=16,\n            expand:int=2, dt_rank:Union[int,str]='auto', d_conv:int=4,\n            pad_vocab_size_multiple:int=8, conv_bias:bool=True,\n            bias:bool=False)"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "model",
    "section": "",
    "text": "Mamba Implementation in JAX Equinox\nSuggest reading the following before/while reading the code: * [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao) https://arxiv.org/abs/2312.00752 * [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti) https://srush.github.io/annotated-s4 * Inspired by https://github.com/johnma2006/mamba-minimal\nGlossary: * b: batch size (B in Mamba paper [1] Algorithm 2) * l: sequence length (L in [1] Algorithm 2) * d or d_model: hidden dim * n or d_state: latent state dim (N in [1] Algorithm 2) * expand: expansion factor (E in [1] Section 3.4) * d_in or d_inner: d * expand (D in [1] Algorithm 2) * A, B, C, D: state space parameters (See any state space representation formula) (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not) * Δ or delta: input-dependent step size * dt_rank: rank of Δ (See [1] Section 3.6 “Parameterization of ∆”)\n\n\nState space models\nState space models(SSMs) give a mapping from a \\(1-D\\) input \\(u(t)\\) onto an \\(N-D\\) latent state \\(x(t)\\), which is then projected down a \\(1-D\\) output \\(y(t)\\).\n\\[\n\\begin{aligned}\nx'(t) &= Ax(t)+ Bu(t) \\\\\ny(t) &= Cx(t) + Du(t)\n\\end{aligned}\n\\] where \\(A,B,C,\\) and \\(D\\) are learnable parameters.\nFor simplicity lets start by omitting the \\(D\\) term. \\(Du\\) can be viewed as an easy to compute skip connection\n\nsource\n\n\nrandom_SSM\n\n random_SSM (rng, N)\n\nTo apply this SSM on a discrete sequence \\((u_0,u_1,u_2,\\dots)\\) instead of the continous function \\(u(t)\\), it must be discretized with step size \\(\\Delta\\) representing input resolution so that input \\(u_k:=u(k\\Delta)\\)\nThis discretization can be performed by a bilinear transformation, mapping the state matrix \\(A\\) to \\(\\bar{A}\\). \\[\n\\begin{aligned}\n\\bar{A} &= (I-\\Delta/2 A)^{-1}(I+\\Delta/2 A) \\\\\n\\bar{B} &= (I-\\Delta/2 A)^{-1}\\Delta B \\\\\n\\bar{C} &= C\n\\end{aligned}\n\\]\n\ndef discretize(A, B, C, step):\n    I = jnp.eye(A.shape[0])\n    BL = jla.inv(I - (step / 2.0) * A)\n    Ab = BL @ (I + (step / 2.0) * A)\n    Bb = (BL * step) @ B\n    return Ab, Bb, C\n\nGiven this sequence to sequence mapping, the discrete SSM can be computed like n RNN with a recurrence relation in \\(x\\). \\[\n\\begin{aligned}\nx_{k} &= \\bar{A}x_{k-1} + \\bar{B}u_k \\\\\ny_k &= \\bar{C}x_k\n\\end{aligned}\n\\]\n\ndef scan_SSM(Ab, Bb, Cb, u, x0):\n    def step(x_k_1, u_k):\n        x_k = Ab @ x_k_1 + Bb @ u_k\n        y_k = Cb @ x_k\n        return x_k, y_k\n\n    return jax.lax.scan(step, x0, u)\n\nTo run the SSM we first discretize and then iterate.\n\ndef run_SSM(A, B, C, u):\n    L = u.shape[0]\n    N = A.shape[0]\n    Ab, Bb, Cb = discretize(A, B, C, step=1.0 / L)\n\n    # Run recurrence\n    return scan_SSM(Ab, Bb, Cb, u[:, np.newaxis], np.zeros((N,)))[1]\n\n|\nModel from Figure 3 of 2312.00752\n\nclass MambaBlock(eqx.Module):\n    args: ModelArgs\n    in_proj: eqx.nn.Linear\n    conv1d: eqx.nn.Conv\n    x_proj: eqx.nn.Linear\n    dt_proj: eqx.nn.Linear\n    out_proj: eqx.nn.Linear\n    log_A: jnp.ndarray\n    D: jnp.ndarray\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n\n        self.in_proj = eqx.nn.Linear(args.d_model, args.d_inner*2, use_bias=args.bias, key = rng)\n\n        self.conv1d = eqx.nn.Conv1D(in_channels=args.d_inner, \n                                 out_channels=args.d_inner, \n                                 kernel_size=args.d_conv,\n                                 groups=args.d_inner,\n                                 use_bias= args.conv_bias,\n                                 key = rng)\n        # maps x to (Δ, B, C)\n        self.x_proj = eqx.nn.Linear(args.d_inner, args.dt_rank+args.d_state*2, use_bias=False, key = rng)\n\n        # projects Δ from dt_rank to d_in\n        self.dt_proj = eqx.nn.Linear(args.dt_rank, args.dt_inner, use_bias=True, key = rng)\n        \n        self.out_proj = eqx.nn.Linear(args.d_inner, args.d_model, use_bias=args.bias, key = rng)\n\n        A = jnp.tile(jnp.arange(1, args.d_state + 1), (args.inner, 1))\n        self.log_A = jnp.log(A)\n        self.D = jnp.ones((args.d_inner, args.d_inner))\n\n    def forward(self, x):\n        \"\"\"\n        Mamba forward pass, looks like figure 3\n        (seq_len, d_model) -&gt; (seq_len, d_model)\n        \"\"\"\n        (L,d) = x.shape\n\n        x_and_res = self.in_proj(x) # (L, d_inner*2)\n        (x, res) = jnp.split(x_and_res, 2, axis=-1) # (L, d_inner)\n        x = self.conv1d(x.T)[:,:,:, :L].T # (L, d_inner)\n        x = jax.nn.silu(x)\n\n        y = self.ssm(x)\n        y *= jax.nn.silu(res)\n        \n        output = self.out_proj(y)\n\n        return output\n    \n    def ssm(self, x):\n        \"\"\"\n        Run SSM\n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        \n        A = -jnp.exp(self.A_log) # (d_in, n)\n        D = self.D\n\n        x_dbl = self.x_proj(x) # (l, d_rank+ d_state*2)\n        (delta, B, C) = jnp.split(x_dbl, [self.args.dt_rank, self.args.d_state,\n                                          self.args.d_state], axis=-1) # (1, d_rank), (l, d_state), (l, d_state)\n        delta = jax.nn.softplus(self.dt_proj(delta)) # (l, d_inner)\n\n        y = self.selective_scan(x, delta, A, B, C, D)\n\n        return y\n\n    def selective_scan(self, u, delta, A, B, C, D):\n        \"\"\" \n        Discretize and Selective scan\n        \"\"\"\n        (L, d_in) = u.shape\n        n = A.shape[1]\n\n        # Discretizing continouis parameters acc eq 4\n        deltaA = jnp.exp(jnp.einsum('ld, dn -&gt; ldn', delta, A))\n        deltaB_u = jnp.einsum('ld,ln, ld-&gt;ldn', delta, B * u)\n\n        # perform selective scan\n        y = scan_SSM(deltaA, deltaB_u, C, u, jnp.zeros((n,)))\n\n        y += u*D\n    \n    def scan_SSM(Ab, Bb, Cb, u, x0):\n        def step(x_k_1, u_k):\n            x_k = Ab @ x_k_1 + Bb @ u_k\n            y_k = Cb @ x_k\n            return x_k, y_k\n\n        return jax.lax.scan(step, x0, u)\n\n\nclass RMSnorm(eqx.Module):\n    eps: float\n    weight: jnp.ndarray\n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = jnp.ones((d_model,))\n\n    def __call__(self, x):\n        output = x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps) * self.weight\n        return output\n\n\nrng = jax.random.PRNGKey(0)\nd_model = 10  # Choose an appropriate size for the test\n\n# Create a random input tensor\nx = jax.random.normal(rng, (5, d_model))\n\n# Instantiate the RMSnorm module\nrmsnorm_module = RMSnorm(d_model)\n\n\nrmsnorm_module(x)\n\nArray([[-0.28202367, -1.7751259 , -1.3276266 ,  0.21005142,  0.00262558,\n        -1.0843129 , -1.1610469 ,  0.17709939, -0.2544334 ,  1.5305542 ],\n       [-0.24652143, -0.7815071 ,  0.41000503,  0.53822225, -1.3681791 ,\n        -2.1942427 , -0.27373582,  1.3746103 ,  0.3120011 ,  0.34948558],\n       [-0.34938094,  0.26039216,  1.1528395 , -0.28252405, -1.6644143 ,\n         1.4754516 , -0.5000238 ,  0.12314829, -1.2202971 , -1.3036996 ],\n       [ 0.81434023,  0.6395449 ,  0.05811654, -0.08327827,  0.42880297,\n         0.22944313, -0.07316481,  1.6281995 ,  2.4002602 ,  0.5130036 ],\n       [-1.188451  , -0.14157107,  0.9150801 ,  1.1505973 , -1.235271  ,\n        -0.4042365 , -1.7946148 , -0.13045768, -1.1099608 , -0.49715152]],      dtype=float32)\n\n\n\nclass ResidualBlock(eqx.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.mixer = MambaBlock(args)\n        self.norm = RMSnorm(args.d_model)\n\n    def forward(self, x):\n        self.mixer(self.norm(x)) + x\n\n\nclass Mamba(eqx.Module):\n    def __init__(self, args: ModelArgs):\n        pass\n\n    def forward(self, x):\n        pass\n\n    @staticmethod\n    def load_pretrained(pretrained_model_name: str):\n        pass"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  }
]